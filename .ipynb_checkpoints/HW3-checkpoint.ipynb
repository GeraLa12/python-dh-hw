{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание №1**\n",
    "\n",
    "Скачайте текст \"Литературных анекдотов\". Напишите функцию, которая будет читать файл, лемматизировать текст с помощью pymystem3 и записывать результат в новый файл. У функции должно бы два аргумента: путь к исходному файлу и путь к файлу с лемматизированным текстом. Вызов функции тоже должен быть прописан в решении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmas(original, lemma_anecdotes): #создаем функцию и два аргумента\n",
    "    f = open(original, 'r', encoding='utf-8') # чтение файла \n",
    "    poem = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    words = poem.split()\n",
    "    no_punctuation1 = [w.strip(punctuation) for w in words] #убираем пунктуацию\n",
    "    no_punctuation2 = [w.strip(\"—«»!...?-\") for w in no_punctuation1] # убираем тире и другую не убранную пунктуацию\n",
    "    cleanpoem = ' '.join(no_punctuation2)\n",
    "    \n",
    "    m = Mystem() #Лемматизация с помощью pymystem3\n",
    "    lemmas = m.lemmatize(cleanpoem)\n",
    "    lemma_poem = ''.join(lemmas)\n",
    "    \n",
    "    with open(lemma_anecdotes, 'w', encoding='utf-8') as f: # записать результат лемматизации в файл\n",
    "        f.write(lemma_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Путь к исходному файлу: literary_anecdotes.txt\n",
      "Имя нового файла с лемматизированным текстом: lemma_anecdotes4.txt\n"
     ]
    }
   ],
   "source": [
    "#вызов функции\n",
    "lemmas(input(\"Путь к исходному файлу: \"), input(\"Имя нового файла с лемматизированным текстом: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Задание №2**\n",
    "\n",
    "Очистите лемматизированный текст от стоп-слов и посчитайте ipm (относительную частоту слов) для оставшихся. Выведите 20 самых частотных по этому параметру слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('stopwords.txt', 'r', encoding='utf-8') # открываем файл со стоп-словами\n",
    "stopwords = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"lemma_anecdotes4.txt\", 'r', encoding='utf-8') # открываем файл с леммами\n",
    "lemmatised_anecdotes = f.read()\n",
    "f.close()\n",
    "no_enter = [w.strip(\"\\n\") for w in lemmatised_anecdotes.split()] # на всякий случай еще обрежем переносы строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['квартира', 'окно', 'тверской', 'бульвар', 'пушкин', 'любить', 'ходить', 'гость', 'прийти', 'сразу', 'прыг', 'подоконник', 'свешиваться', 'окно', 'смотреть', 'чай', 'тоже', 'туда', 'окно']\n"
     ]
    }
   ],
   "source": [
    "#очищаем леммы от стоп слов и ненужных пробелов\n",
    "stopwords = stopwords.split('\\n')\n",
    "clean_lemmas = [i for i in no_enter if i not in stopwords and i != \" \" and i != \"  \"] \n",
    "print(clean_lemmas [1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это абсолютная частота \n",
    "count_clean_lemmas = Counter(clean_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем ipm (относительную частоту слов) для оставшихся лемм \n",
    "# это отношение его абсолютной частоты к какой-нибудь другой величине, например, к длине текста. \n",
    "# это отношение абсолютной частоты какого-либо элемента к объему корпуса, умноженное на миллион\n",
    "\n",
    "ipm = {} #создаем словарь, куда будем считать частоту\n",
    "\n",
    "for k,v in count_clean_lemmas.items():\n",
    "    ipm[k] = v / len (lemmatised_anecdotes)  * 1000000 # lemmatised_anecdotes - это лемматизированный текст, еще не очищенный от стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отсортируем по частоте \n",
    "sorted_counts = sorted(ipm.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('пушкин', 3401.164466478354), ('толстой', 1902.3462270133164), ('гоголь', 1787.0525162852366), ('однажды', 1671.7588055571568), ('лев', 1441.1713841009973), ('любить', 1152.9371072807978), ('достоевский', 1152.9371072807978), ('тургенев', 922.3496858246382), ('ребенок', 864.7028304605983), ('царствие', 864.7028304605983), ('небесный', 864.7028304605983), ('окно', 691.7622643684787), ('тверской', 691.7622643684787), ('бульвар', 691.7622643684787), ('приходить', 691.7622643684787), ('лермонтов', 691.7622643684787), ('федор', 634.1154090044388), ('михайлович', 634.1154090044388), ('идти', 576.4685536403989), ('герцен', 576.4685536403989)]\n"
     ]
    }
   ],
   "source": [
    "# выведем 20 самых частотных по этому параметру слов \n",
    "print(sorted_counts[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('пушкин', 59), ('толстой', 33), ('гоголь', 31), ('однажды', 29), ('лев', 25), ('любить', 20), ('достоевский', 20), ('тургенев', 16), ('ребенок', 15), ('царствие', 15), ('небесный', 15), ('окно', 12), ('тверской', 12), ('бульвар', 12), ('приходить', 12), ('лермонтов', 12), ('федор', 11), ('михайлович', 11), ('идти', 10), ('герцен', 10)]\n"
     ]
    }
   ],
   "source": [
    "# а тут 20 самых частотных, если считать по абсолютной частоте\n",
    "count_clean_lemmas = Counter(clean_lemmas)\n",
    "print(count_clean_lemmas.most_common(20)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание №3**\n",
    "\n",
    "Сделайте полный морфологический разбор исходного текста. Напишите регулярное выражение, которое будет извлекать из тега только часть речи. Пройдитесь циклом по списку с разборами, который выдал pymystem3, извлекая из каждого разбора форму слова и его часть речи и записывая их в новый словарь (форма -- ключ, часть речи -- значение). Посчитайте абсолютную частоту для всех частей речи, а затем относительнную частоту (абсолютная частота / длина текста в словах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymystem3 import Mystem\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('literary_anecdotes.txt', 'r', encoding='utf-8') #открываем изначальный файл\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# производим анализ текста \n",
    "m = Mystem()\n",
    "analysis_anecdotes = m.analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = {} # словарь, содержащий только грамматику\n",
    "for word in analysis_anecdotes:\n",
    "    try:\n",
    "        form = word[\"text\"]\n",
    "        grammar = word [\"analysis\"] [0] [\"gr\"]\n",
    "        words[form] = grammar\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = re.compile('[A-Z]+') # регулярное выражение, которое будет извлекать из тега только части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_pos = {} # пустой словарь, чтобы туда записывались значения\n",
    "\n",
    "for k,v in words.items():\n",
    "    words_pos[k] = pos.match(v).group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте абсолютную частоту для всех частей речи, а затем относительнную частоту (абсолютная частота / длина текста в словах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PR': 37, 'S': 440, 'V': 436, 'A': 100, 'ADV': 87, 'SPRO': 54, 'CONJ': 20, 'PART': 28, 'ADVPRO': 24, 'APRO': 51, 'ANUM': 2, 'INTJ': 7, 'NUM': 9}\n"
     ]
    }
   ],
   "source": [
    "# посчитаем абсолютную частоту. \n",
    "# Создаем словарь, в котором при каждом проходе будет считаться количество частей речи\n",
    "abs_freq = {}\n",
    "\n",
    "for v in words_pos.values():\n",
    "    if v in abs_freq:\n",
    "        abs_freq [v] += 1\n",
    "    else:\n",
    "        abs_freq [v] = 1\n",
    "print(abs_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PR': 13028.169014084508, 'S': 154929.57746478872, 'V': 153521.12676056338, 'A': 35211.2676056338, 'ADV': 30633.80281690141, 'SPRO': 19014.08450704225, 'CONJ': 7042.2535211267605, 'PART': 9859.154929577466, 'ADVPRO': 8450.704225352112, 'APRO': 17957.74647887324, 'ANUM': 704.2253521126761, 'INTJ': 2464.7887323943664, 'NUM': 3169.0140845070423}\n"
     ]
    }
   ],
   "source": [
    "# относительная частота\n",
    "\n",
    "ipm_pos_freq = {}\n",
    "\n",
    "for k,v in abs_freq.items():\n",
    "    ipm_pos_freq[k] = v / len (text.split()) * 1000000\n",
    "print (ipm_pos_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
